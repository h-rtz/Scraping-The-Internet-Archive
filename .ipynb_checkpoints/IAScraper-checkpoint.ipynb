{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Internet Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mich interessiert, welche Artikel wann auf der Startseite der NZZ ausgespielt wurden, und zwar retrospektiv für das Jahr 2020 oder sogar noch länger zurück. \n",
    "\n",
    "Snapshots (auch 'Captures') von www.nzz.ch sind im Internet Archive hinterlegt. Ich baue einen Scraper, der diese Snapshots abspeichert und eine Tabelle erstellt mit Titeln, Teasern und URL. Anschliessend durchsuche ich diese Datenbank nach bestimmten Schlagworten.\n",
    "\n",
    "Zum Beispiel können wir mit diesem Datensatz\n",
    "* die mediale Karriere von Trump nachzeichnen: Wann beherrschte er die News? Und weshalb?\n",
    "* das Coronajahr analysieren: Wann standen Tests, Impfungen, Lockdown, Kinder & Homeschoooling, Homeoffice, Psyche und Krise im Vordergrund?\n",
    "\n",
    "Das Scraping findet in drei Stufen statt:\n",
    "1. Wir erstellen eine Liste der URLs der einzelnen Snapshots eines Jahres. \n",
    "2. Wir rufen die URLs auf der Liste mit dem Scraper auf und speichern das html ab (eines pro Capture)\n",
    "3. Wir erstellen aus dem html eine Liste mit Titeln, Teasern und Artikel-URL und speichern diese ebenfalls ab (auch hier eines pro Capture)\n",
    "\n",
    "Die Analyse findet im Notebook CoronaTrendsSchweiz.ipynb statt:\n",
    "1. Suche in den Listen aus Scraping-3 nach passenden Schlagworten und schreibe diese in eine Liste (Datum - Trefferzahl)\n",
    "2. Visualisiere als Steamplot\n",
    "3. Füge bei Ereignissen eine Annotation an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from time import sleep\n",
    "from time import time\n",
    "from random import randint\n",
    "from warnings import warn\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Liste aller Captures der NZZ-Frontpage im Internet Archive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der URL aller NZZ captures in der Wayback machine.\n",
    "# Dafür gibt es eine API: https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server\n",
    "\n",
    "# Suche via API-Abfrage auf 2020 eingrenzen\n",
    "url = 'http://web.archive.org/cdx/search/cdx?url=nzz.ch&from=20200101&to=20201231&output=json'   \n",
    "urls = rq.get(url).text\n",
    "\n",
    "# Aus dem json müssen wir die Timestamps extrahieren und diese dann wieder zur URL des Captures zusammenfügen:\n",
    "parse_url = json.loads(urls) \n",
    "\n",
    "url_list = []\n",
    "for i in range(1,len(parse_url)):\n",
    "    tstamp = parse_url[i][1]\n",
    "    waybacklink = 'https://web.archive.org/web/'+ tstamp +'/https://www.nzz.ch/'\n",
    "    url_list.append(waybacklink)\n",
    "\n",
    "# alle erzeugten URL in ein txt-file schreiben:    \n",
    "with open('IA_captures_nzz_fp.txt', 'w') as filehandle:\n",
    "    json.dump(url_list, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7078"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Code zum Scrapen der Captures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML aufrufen, parsen, abspeichern \n",
    "\n",
    "for url in url_list[320:]:\n",
    "    page = rq.get(url).text\n",
    "    soup = bs(page, 'html.parser')\n",
    "    html = soup.prettify(\"utf-8\")\n",
    "\n",
    "    timestamp = url.split('/')[4]\n",
    "    with open(f\"htmls/NZZ-frontpage-{timestamp}.html\", \"wb\") as file:\n",
    "        file.write(html)\n",
    "\n",
    "# Titel, Teaser und url aller Artikel herausziehen\n",
    "    \n",
    "    articles = soup.find_all('div', {'class' : \"teaser__content\"})\n",
    "    data = []\n",
    "    for article in articles:\n",
    "        url = article.find('a')[\"href\"].split('//')[1]\n",
    "        try:\n",
    "            title = article.find('h2', {'class' : \"teaser__title\"}).text.replace(\"\\n\", \"\")\n",
    "        except:\n",
    "            title = 'n/a'\n",
    "        try:\n",
    "            teaser = article.find('div', {'class' : \"teaser__lead\"}).text.replace(\"\\n\", \"\")\n",
    "        except:\n",
    "            teaser = 'n/a'\n",
    "        \n",
    "        mini_dict = {'Titel': title,\n",
    "                     'Teaser': teaser,\n",
    "                     'url': url}\n",
    "        data.append(mini_dict)\n",
    "        pd.DataFrame(data).to_csv(f'csvs/{timestamp}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
